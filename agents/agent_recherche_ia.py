"""
Agent 1 - Recherche Web IA
Mod√®le : GPT-5.2 (OpenAI)
R√¥le : Collecte factuelle d'informations depuis sites institutionnels IA
Sans interpr√©tation ni analyse - Restitution brute : cat√©gorie, titre, r√©sum√©, synth√®se, source+lien
"""

import os
import sys
import json
import hashlib
import traceback
from datetime import datetime, timedelta
from typing import Dict, Any
from openai import OpenAI


# ================================================================================
# CONFIGURATION
# ================================================================================

OPENAI_API_KEY = os.environ.get('OPENAI_API_KEY')

# Mod√®le GPT-5.2 pour recherche web avec web_search tool
MODEL_RECHERCHE = "gpt-5.2"

# Fichier de sortie
OUTPUT_JSON = "recherche_ia_brute.json"

# Sources institutionnelles IA (d√©finies par Nicolas)
SOURCES_IA = [
    "https://www.anthropic.com",  # Anthropic (Claude)
    "https://openai.com",          # OpenAI (GPT)
    "https://mistral.ai",          # Mistral AI (France)
    "https://www.deepseek.com",    # DeepSeek (Chine)
    "https://thehackernews.com",   # The Hacker News
    "https://www.deeplearning.ai", # DeepLearning.AI
    "https://ai.google",           # Google AI (ajout institutionnel)
    "https://www.nvidia.com/en-us/ai/" # NVIDIA AI (ajout institutionnel)
]


# ================================================================================
# RECHERCHE WEB AVEC GPT-5.2
# ================================================================================

def rechercher_actualites_ia() -> Dict[str, Any]:
    """
    Utilise GPT-5.2 avec web_search tool (live web access) pour collecter
    les actualit√©s factuelles depuis les sources institutionnelles IA.
    
    Returns:
        Dictionnaire JSON avec articles
    """
    
    if not OPENAI_API_KEY:
        print("‚ùå OPENAI_API_KEY manquante")
        return {"articles": []}
    
    print(f"üîç Cr√©ation client OpenAI pour recherche web...")
    client = OpenAI(api_key=OPENAI_API_KEY)
    
    # Calculer p√©riode (7 derniers jours)
    date_fin = datetime.now()
    date_debut = date_fin - timedelta(days=7)
    
    # Pr√©parer liste sources
    sources_text = "\n".join([f"- {source}" for source in SOURCES_IA])
    
    # Construire prompt de recherche factuelle
    prompt = f"""Tu es un collecteur d'informations factuelles sur l'Intelligence Artificielle.

**P√âRIODE** : du {date_debut.strftime('%d/%m/%Y')} au {date_fin.strftime('%d/%m/%Y')}

**SOURCES PRIORITAIRES √Ä CONSULTER** :
{sources_text}

**TA MISSION - COLLECTE FACTUELLE UNIQUEMENT** :
1. Recherche les actualit√©s IA/LLM publi√©es cette semaine sur ces sources institutionnelles
2. Pour chaque actualit√© trouv√©e, extrais UNIQUEMENT les faits :
   - Titre exact de l'article
   - R√©sum√© court (2-3 lignes) - FACTS ONLY, pas d'interpr√©tation
   - Contenu factuel complet de l'article (qui, quoi, quand, o√π)
   - Source exacte (nom du site)
   - URL compl√®te de l'article
   - Date de publication estim√©e

3. Cat√©gorise chaque actualit√© dans l'un de ces th√®mes :
   - "Nouveaux mod√®les LLM" (lancements, versions, benchmarks)
   - "Agents autonomes" (AutoGPT, frameworks agentiques)
   - "Multimodal" (vision, audio, vid√©o)
   - "Reasoning" (o1, o3, R1, cha√Æne de pens√©e)
   - "Open source" (Llama, Mistral, communaut√©)
   - "Recherche" (papiers ArXiv, conf√©rences)
   - "R√©gulation" (AI Act, gouvernance)
   - "Safety" (alignment, risques)
   - "Industrie" (lev√©es de fonds, acquisitions)
   - "Hardware" (GPU, TPU, Groq)
   - "France/Europe" (acteurs fran√ßais, r√©gulation UE)
   - "Asie" (Chine, DeepSeek, Baidu)

**FORMAT DE SORTIE JSON - STRUCTURE OBLIGATOIRE** :
R√©ponds UNIQUEMENT avec un JSON valide suivant ce format exact :

{{
  "articles": [
    {{
      "categorie": "Nouveaux mod√®les LLM",
      "titre": "Titre exact de l'article",
      "resume_court": "R√©sum√© factuel en 2-3 lignes maximum",
      "synthese_complete": "Contenu complet factuel",
      "source": "Nom du site (ex: Anthropic, OpenAI)",
      "url": "https://url-complete.com",
      "date_publication": "2026-02-01"
    }}
  ],
  "periode": {{
    "debut": "{date_debut.strftime('%Y-%m-%d')}",
    "fin": "{date_fin.strftime('%Y-%m-%d')}"
  }},
  "sources_consultees": ["Anthropic", "OpenAI"]
}}

**CONSIGNES CRITIQUES** :
- Recherche 10-15 actualit√©s maximum
- UNIQUEMENT des faits v√©rifiables
- AUCUNE interpr√©tation, analyse, opinion
- Citations exactes quand pertinent
- URLs compl√®tes obligatoires

**IMPORTANT** :
Tu es un COLLECTEUR, pas un ANALYSTE. Tu ne portes AUCUN jugement.
Utilise web_search pour acc√©der aux sites institutionnels.
G√©n√®re le JSON maintenant, sans pr√©ambule."""

    print("üåê Lancement recherche web GPT-5.2 avec web_search (LIVE WEB)...")
    
    try:
        # Appel API GPT-5.2 avec Responses API + web_search tool + LIVE WEB
        response = client.responses.create(
            model=MODEL_RECHERCHE,
            tools=[{"type": "web_search", "external_web_access": True}],  # LIVE WEB = True
            input=prompt,
            max_tokens=8000  # Sp√©cifi√© par Nicolas
        )
        
        print(f"üìä Tokens utilis√©s : {response.usage.total_tokens} (prompt: {response.usage.prompt_tokens}, completion: {response.usage.completion_tokens})")
        
        # Co√ªt GPT-5.2 (estimation, √† v√©rifier)
        cost_input = (response.usage.prompt_tokens / 1000) * 0.01
        cost_output = (response.usage.completion_tokens / 1000) * 0.03
        cost_total = cost_input + cost_output
        print(f"üí∞ Co√ªt estim√© : ${cost_total:.4f}")
        
        # Extraire JSON depuis output_text
        json_text = response.output_text.strip()
        
        # Nettoyer les backticks markdown si pr√©sents
        if json_text.startswith('```'):
            lines = json_text.split('\n')
            json_text = '\n'.join(lines[1:-1]) if len(lines) > 2 else json_text
            json_text = json_text.replace('```json', '').replace('```', '').strip()
        
        print(f"üìù Parsing JSON ({len(json_text)} caract√®res)...")
        
        data = json.loads(json_text)
        
        # Ajouter m√©tadonn√©es
        data['date_collecte'] = date_fin.strftime('%Y-%m-%d')
        data['model_utilise'] = MODEL_RECHERCHE
        data['agent'] = "Recherche IA"
        
        # V√©rifier structure
        if 'periode' not in data:
            data['periode'] = {
                'debut': date_debut.strftime('%Y-%m-%d'),
                'fin': date_fin.strftime('%Y-%m-%d')
            }
        
        # G√©n√©rer IDs uniques
        for article in data.get('articles', []):
            hash_input = f"{article.get('url', '')}{article.get('titre', '')}"
            article['id'] = hashlib.md5(hash_input.encode()).hexdigest()[:12]
        
        print(f"‚úÖ Recherche termin√©e : {len(data.get('articles', []))} articles collect√©s")
        
        return data
    
    except json.JSONDecodeError as e:
        print(f"‚ùå Erreur parsing JSON : {e}")
        print(f"R√©ponse brute (premiers 500 car) : {json_text[:500]}...")
        raise
    
    except Exception as e:
        print(f"‚ùå Erreur GPT-5.2 : {e}")
        traceback.print_exc()
        raise


# ================================================================================
# SAUVEGARDE JSON
# ================================================================================

def sauvegarder_json(data: Dict[str, Any], filepath: str) -> None:
    """
    Sauvegarde le JSON de recherche brute
    
    Args:
        data: Donn√©es √† sauvegarder
        filepath: Chemin du fichier
    """
    print(f"üíæ Sauvegarde du JSON dans {filepath}...")
    
    with open(filepath, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    
    file_size = os.path.getsize(filepath)
    print(f"‚úÖ JSON sauvegard√© : {filepath}")
    print(f"üìä Taille : {file_size} octets ({file_size / 1024:.2f} KB)")


# ================================================================================
# MAIN
# ================================================================================

def main():
    """Point d'entr√©e principal de l'agent recherche IA"""
    
    try:
        print("=" * 80)
        print("ü§ñ AGENT 1 - RECHERCHE WEB IA (GPT-5.2 + LIVE WEB SEARCH)")
        print("=" * 80)
        print(f"‚è∞ Ex√©cution : {datetime.now().strftime('%d/%m/%Y %H:%M:%S')}")
        print(f"üìÇ R√©pertoire de travail : {os.getcwd()}")
        print()
        
        # V√©rifier cl√© API
        print("üîë V√©rification cl√© API...")
        if not OPENAI_API_KEY:
            print("‚ùå ERREUR CRITIQUE : OPENAI_API_KEY manquante")
            sys.exit(1)
        else:
            print(f"‚úÖ OPENAI_API_KEY pr√©sente ({OPENAI_API_KEY[:10]}...)")
        
        print()
        
        # Recherche web
        print("üì° RECHERCHE WEB FACTUELLE IA (GPT-5.2)")
        print("-" * 80)
        print("Sources institutionnelles :")
        for source in SOURCES_IA:
            print(f"  ‚Ä¢ {source}")
        print()
        
        data = rechercher_actualites_ia()
        print()
        
        # Sauvegarde
        print("üíæ SAUVEGARDE JSON")
        print("-" * 80)
        sauvegarder_json(data, OUTPUT_JSON)
        print()
        
        # R√©sum√©
        print("=" * 80)
        print("‚úÖ AGENT 1 RECHERCHE IA TERMIN√â")
        print("=" * 80)
        print(f"üìä {len(data.get('articles', []))} articles collect√©s")
        print(f"üìÇ Fichier JSON : {OUTPUT_JSON}")
        print(f"üîó Pr√™t pour Agent 3 (Synth√®se IA)")
        print()
        
        sys.exit(0)
    
    except KeyboardInterrupt:
        print("\n‚ö†Ô∏è  Interruption manuelle (Ctrl+C)")
        sys.exit(130)
    
    except Exception as e:
        print("\n" + "=" * 80)
        print("‚ùå ERREUR FATALE")
        print("=" * 80)
        print(f"Type : {type(e).__name__}")
        print(f"Message : {e}")
        print("\nTraceback :")
        traceback.print_exc()
        print("=" * 80)
        sys.exit(1)


if __name__ == "__main__":
    main()
