"""
Agent de Veille IA avec Recherche Web
Mod√®le : GPT-5.2 avec web_search
R√¥le : Recherche web sur actualit√©s IA/LLM ‚Üí Analyse ‚Üí Synth√®se Markdown
Budget estim√© : ~0.10‚Ç¨ par ex√©cution
"""

import os
import sys
import traceback
from datetime import datetime, timedelta
from typing import List, Tuple
from openai import OpenAI


# ================================================================================
# CONFIGURATION
# ================================================================================

OPENAI_API_KEY = os.environ.get('OPENAI_API_KEY')

# Mod√®le GPT-5.2 avec web search
MODEL_GPT52 = "gpt-5.2"

# Fichier de sortie
OUTPUT_MARKDOWN = "research_ia.md"

# Timeout (recherches longues)
REQUEST_TIMEOUT = 600  # 10 minutes

# Limite tokens de sortie ‚Äî 4000 pour couvrir 15-20 articles avec r√©sum√©s
MAX_OUTPUT_TOKENS = 4000


# ================================================================================
# PROMPT RECHERCHE WEB IA
# ================================================================================

def generer_prompt_recherche() -> str:
    """
    G√©n√®re le prompt pour recherche web IA avec synth√®se.

    Returns:
        Prompt optimis√© pour recherche web + analyse + synth√®se
    """

    date_fin = datetime.now()
    date_debut = date_fin - timedelta(days=7)

    prompt = f"""Tu es un analyste expert en IA/LLM. Ta mission comporte 3 √©tapes :

√âTAPE 1 : RECHERCHE WEB
Utilise l'outil de recherche web pour trouver des articles R√âELS et R√âCENTS sur l'IA/LLM.
N'invente JAMAIS d'URLs fictives.

√âTAPE 2 : ANALYSE
Analyse les articles trouv√©s pour identifier les plus pertinents.

√âTAPE 3 : SYNTH√àSE MARKDOWN
G√©n√®re un document Markdown structur√© avec les articles s√©lectionn√©s.

P√âRIM√àTRE G√âOGRAPHIQUE :
- √âtats-Unis (OpenAI, Anthropic, Meta, Google)
- Europe (Mistral AI France, startups europ√©ennes)
- Asie (DeepSeek Chine, entreprises asiatiques)
- **FOCUS SP√âCIAL** : IA √† Nantes et en Bretagne (startups, √©cosyst√®me local, √©v√©nements)

SOURCES PRIORITAIRES :
- **Blogs officiels** : OpenAI Blog, Anthropic Blog, Google AI Blog, Meta AI Blog
- **Publications √©diteurs** : Mistral AI, Hugging Face, Stability AI
- **Recherche acad√©mique** : ArXiv, Papers with Code, conf√©rences (NeurIPS, ICML)
- **Communiqu√©s officiels** : annonces produits, lev√©es de fonds
- **M√©dias tech** : TechCrunch, The Verge, Wired, VentureBeat
- **√âviter** : agr√©gateurs secondaires, contenus marketing, republications

TH√àMES √Ä COUVRIR :
1. Nouveaux mod√®les LLM (GPT, Claude, Gemini, Llama, Mistral, DeepSeek)
2. Agents autonomes et Agentic AI
3. Multimodal AI (vision, audio, vid√©o)
4. Reasoning models (o1, o3, R1, chain-of-thought)
5. Open source et √©cosyst√®mes (Hugging Face, communaut√©)
6. Recherche scientifique (papers ArXiv, conf√©rences)
7. R√©gulation et gouvernance (AI Act Europe, l√©gislations)
8. Safety, Alignment, risques IA
9. Investissements et industrie (lev√©es de fonds, acquisitions)
10. Hardware IA (NVIDIA, AMD, TPU, Groq, puces sp√©cialis√©es)
11. **Startups fran√ßaises et europ√©ennes** (focus Mistral AI, Poolside, etc.)
12. **IA Nantes et Bretagne** : √©cosyst√®me local, startups, √©v√©nements, recherche

STRAT√âGIE DE RECHERCHE :
1. Effectue 15-20 recherches web cibl√©es sur les th√®mes ci-dessus
2. Pour chaque th√®me, cherche "actualit√© [th√®me] derni√®re semaine"
3. V√©rifie la date de publication des articles trouv√©s
4. Priorise les sources officielles et les annonces r√©centes
5. Pour Nantes/Bretagne : "actualit√© IA Nantes", "startup IA Bretagne", etc.

CRIT√àRES DE S√âLECTION :
- Actualit√© des 7 derniers jours PRIORITAIRE
- Accepter analyses/rapports r√©cents sur √©v√©nements plus anciens si tr√®s pertinents
- EXCLURE : contenu republi√©/recycl√©, annonces marketing mineures, tutoriels basiques
- PRIVIL√âGIER : vraies nouveaut√©s, annonces officielles, r√©sultats de recherche, sources primaires
- **CRITICAL** : TOUTES les URLs DOIVENT √™tre R√âELLES (trouv√©es par web search)

P√âRIODE ANALYS√âE : du {date_debut.strftime('%d/%m/%Y')} au {date_fin.strftime('%d/%m/%Y')}

FORMAT DE SORTIE MARKDOWN :

```markdown
# Veille IA - Recherche Web
Date : {date_fin.strftime('%Y-%m-%d')}
P√©riode : {date_debut.strftime('%d/%m/%Y')} - {date_fin.strftime('%d/%m/%Y')}

## Articles identifi√©s

### [TITRE ARTICLE 1]
- **Source** : [Nom m√©dia ou blog officiel]
- **URL** : [URL compl√®te R√âELLE trouv√©e via web search]
- **Date** : [Date publication R√âELLE]
- **Th√®me** : [Th√®me principal parmi les 12 ci-dessus]
- **R√©sum√©** : [3-4 lignes synth√©tiques reformul√©es]
- **Pertinence** : [Score 1-10]
- **Tags** : [tag1, tag2, tag3]
- **Zone g√©o** : [USA/Europe/Asie/France/Nantes-Bretagne]

### [TITRE ARTICLE 2]
[...]

[R√©p√©ter pour TOUS les articles trouv√©s - viser 15-20 articles minimum]

## Statistiques de la recherche
- Nombre total d'articles : [X]
- P√©riode couverte : {date_debut.strftime('%d/%m/%Y')} √† {date_fin.strftime('%d/%m/%Y')}
- Nombre de recherches web effectu√©es : [X]
- R√©partition th√©matique :
  - Nouveaux mod√®les : [X]
  - Agents : [X]
  - Multimodal : [X]
  - Reasoning : [X]
  - Open source : [X]
  - Recherche : [X]
  - R√©gulation : [X]
  - Safety : [X]
  - Investissements : [X]
  - Hardware : [X]
  - France/Europe : [X]
  - Nantes/Bretagne : [X]
- R√©partition g√©ographique :
  - USA : [X]
  - Europe : [X]
  - Asie : [X]
  - France : [X]
  - Nantes/Bretagne : [X]
- Sources utilis√©es : [Liste des principaux m√©dias/blogs]
```

CONSIGNES CRITIQUES :
- UTILISE LA RECHERCHE WEB pour CHAQUE th√®me important
- Vise 15-20 articles de haute qualit√© MINIMUM
- Reformule TOUS les r√©sum√©s (JAMAIS de copier-coller)
- **URLs compl√®tes R√âELLES OBLIGATOIRES** (trouv√©es via web search)
- **N'INVENTE JAMAIS d'URLs** - si tu n'as pas trouv√© d'article r√©cent, indique-le
- Score pertinence strict : 9-10 = exceptionnel, 7-8 = important, 5-6 = int√©ressant, <5 = √† filtrer
- Privil√©gie sources originales (blogs officiels OpenAI/Anthropic/Mistral, papers ArXiv, communiqu√©s)
- Pour Nantes/Bretagne : chercher startups locales, √©v√©nements IA, initiatives r√©gionales
- √âquilibre g√©ographique : 50% USA, 30% Europe, 15% Asie, 5% Nantes/Bretagne

Effectue maintenant :
1. RECHERCHE WEB (15-20 recherches)
2. ANALYSE des r√©sultats
3. SYNTH√àSE au format Markdown avec URLs R√âELLES
"""

    return prompt


# ================================================================================
# EXTRACTION DES CITATIONS R√âELLES depuis response.output
# ================================================================================

def extraire_citations(response) -> List[Tuple[str, str]]:
    """
    Extrait les url_citation annotations depuis les sorties du mod√®le.
    Ces citations sont les URLs r√©elles r√©cup√©r√©es par le web_search ‚Äî
    elles sont fiables contrairement aux URLs que le mod√®le peut inventer
    dans le texte g√©n√©r√©.

    Args:
        response: Objet de r√©ponse OpenAI responses.create()

    Returns:
        Liste de tuples (titre, url) extraits des annotations
    """
    citations: List[Tuple[str, str]] = []

    for item in response.output:
        # Les messages de type 'message' contiennent les annotations
        if hasattr(item, 'content') and item.content:
            for block in item.content:
                if hasattr(block, 'annotations') and block.annotations:
                    for annotation in block.annotations:
                        if (
                            hasattr(annotation, 'type')
                            and annotation.type == 'url_citation'
                            and hasattr(annotation, 'url')
                            and hasattr(annotation, 'title')
                        ):
                            citations.append((annotation.title, annotation.url))

    return citations


def injecter_citations_dans_markdown(markdown: str, citations: List[Tuple[str, str]]) -> str:
    """
    Ajoute une section "Sources v√©rifi√©es" √† la fin du markdown avec
    les URLs r√©elles extraites par le web_search. Cette section sert
    de r√©f√©rence fiable si le mod√®le a invent√© des URLs dans le texte.

    Args:
        markdown: Contenu markdown g√©n√©r√© par le mod√®le
        citations: Liste de tuples (titre, url) issus des annotations

    Returns:
        Markdown enrichi avec la section sources v√©rifi√©es
    """
    if not citations:
        print("‚ö†Ô∏è  Aucune citation extraite des annotations ‚Äî v√©rifie que web_search s'est bien activ√©")
        return markdown

    # D√©dupliquer par URL
    seen_urls: set = set()
    unique_citations: List[Tuple[str, str]] = []
    for titre, url in citations:
        if url not in seen_urls:
            seen_urls.add(url)
            unique_citations.append((titre, url))

    # Construire la section
    section = "\n\n## Sources v√©rifi√©es (extraites par web_search)\n"
    section += f"*{len(unique_citations)} URLs r√©elles r√©cup√©r√©es par le moteur de recherche*\n\n"
    for titre, url in unique_citations:
        section += f"- [{titre}]({url})\n"

    print(f"üìé {len(unique_citations)} citations r√©elles inject√©es dans le markdown")
    return markdown + section


# ================================================================================
# RECHERCHE WEB AVEC GPT-5.2
# ================================================================================

def executer_recherche_web() -> str:
    """
    Lance une recherche web via GPT-5.2, analyse et synth√©tise.
    Extrait les citations r√©elles depuis les annotations de la r√©ponse.

    Returns:
        Markdown structur√© avec articles trouv√©s et sources v√©rifi√©es
    """

    if not OPENAI_API_KEY:
        raise ValueError("‚ùå OPENAI_API_KEY manquante")

    print("ü§ñ Initialisation client OpenAI...")
    client = OpenAI(api_key=OPENAI_API_KEY)

    prompt = generer_prompt_recherche()

    print(f"üîç Lancement recherche web GPT-5.2 (timeout {REQUEST_TIMEOUT}s)...")
    print("‚è≥ Cette recherche peut prendre 2-4 minutes...")
    print("üåê Web search activ√© pour URLs r√©elles")
    print("üìä √âtapes : Recherche ‚Üí Analyse ‚Üí Synth√®se ‚Üí Extraction citations")

    try:
        # Appel API responses.create() avec web_search
        # ‚ö†Ô∏è  temperature n'est PAS un param√®tre valide sur responses.create()
        # Le comportement est contr√¥l√© par reasoning.effort
        response = client.responses.create(
            model=MODEL_GPT52,
            input=prompt,
            max_output_tokens=MAX_OUTPUT_TOKENS,
            reasoning={"effort": "medium"},  # √âquilibre qualit√© / co√ªt / latence
            tools=[{"type": "web_search"}],
            # Localisation pour contextualiser les recherches
            # Syntaxe selon doc : param√®tre au niveau racine de l'appel
        )

        # R√©cup√©ration du texte g√©n√©r√©
        markdown_content = response.output_text.strip()

        # Nettoyer les backticks markdown si pr√©sents
        if markdown_content.startswith('```markdown'):
            lines = markdown_content.split('\n')
            markdown_content = '\n'.join(lines[1:-1]) if len(lines) > 2 else markdown_content
            markdown_content = markdown_content.replace('```markdown', '').replace('```', '').strip()

        # Extraction des citations r√©elles depuis les annotations
        citations = extraire_citations(response)
        print(f"üîó Citations extraites des annotations : {len(citations)}")

        # Injection de la section sources v√©rifi√©es
        markdown_content = injecter_citations_dans_markdown(markdown_content, citations)

        print(f"‚úÖ Recherche et synth√®se termin√©es")
        print(f"üìä Tokens g√©n√©r√©s : {response.usage.output_tokens}")
        print(f"üìù Markdown g√©n√©r√© : {len(markdown_content)} caract√®res")

        return markdown_content

    except Exception as e:
        print(f"‚ùå Erreur lors de la recherche : {e}")
        traceback.print_exc()
        raise


# ================================================================================
# SAUVEGARDE MARKDOWN
# ================================================================================

def sauvegarder_markdown(contenu: str, filepath: str) -> None:
    """
    Sauvegarde le Markdown g√©n√©r√©.

    Args:
        contenu: Contenu Markdown
        filepath: Chemin du fichier
    """
    print(f"üíæ Sauvegarde dans {filepath}...")

    with open(filepath, 'w', encoding='utf-8') as f:
        f.write(contenu)

    file_size = os.path.getsize(filepath)
    print(f"‚úÖ Fichier sauvegard√© : {filepath}")
    print(f"üìä Taille : {file_size} octets ({file_size / 1024:.2f} KB)")


# ================================================================================
# MAIN
# ================================================================================

def main():
    """Point d'entr√©e principal"""

    try:
        print("=" * 80)
        print("üîç VEILLE IA - GPT-5.2 avec Recherche Web")
        print("=" * 80)
        print(f"‚è∞ Ex√©cution : {datetime.now().strftime('%d/%m/%Y %H:%M:%S')}")
        print(f"üìÇ R√©pertoire : {os.getcwd()}")
        print()

        if not OPENAI_API_KEY:
            print("‚ùå ERREUR : OPENAI_API_KEY manquante")
            sys.exit(1)

        print("üîç √âTAPE 1/2 : Recherche web + Analyse + Synth√®se")
        print("-" * 80)
        markdown = executer_recherche_web()
        print()

        print("üíæ √âTAPE 2/2 : Sauvegarde du r√©sultat")
        print("-" * 80)
        sauvegarder_markdown(markdown, OUTPUT_MARKDOWN)
        print()

        print("=" * 80)
        print("‚úÖ VEILLE IA TERMIN√âE")
        print("=" * 80)
        print(f"üìÑ Fichier : {OUTPUT_MARKDOWN}")
        print(f"üîó Pr√™t pour agent de mise en forme")
        print(f"‚úÖ Citations r√©elles extraites des annotations web_search")
        print()

        sys.exit(0)

    except Exception as e:
        print("\n" + "=" * 80)
        print("‚ùå ERREUR FATALE")
        print("=" * 80)
        print(f"Type : {type(e).__name__}")
        print(f"Message : {e}")
        traceback.print_exc()
        print("=" * 80)
        sys.exit(1)


if __name__ == "__main__":
    main()
