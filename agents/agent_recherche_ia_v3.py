"""
Agent Recherche IA v3 - Collecte PURE
Mod√®le : GPT-5.2 (OpenAI Responses API)
R√¥le : Collecte factuelle brute SANS tri, SANS analyse, SANS synth√®se
Note : GPT-5.2 ne supporte pas max_tokens dans responses.create()
"""

import os
import sys
import json
import hashlib
import traceback
from datetime import datetime, timedelta
from typing import Dict, Any
from openai import OpenAI

# Configuration
OPENAI_API_KEY = os.environ.get('OPENAI_API_KEY')
MODEL_RECHERCHE = "gpt-5.2"
OUTPUT_JSON = "recherche_ia_brute.json"
MAX_ARTICLES = 25

def collecter_actualites_ia() -> Dict[str, Any]:
    """
    Collecte brute des actualit√©s IA via web search.
    AUCUNE analyse, AUCUN tri, AUCUNE synth√®se.
    """
    if not OPENAI_API_KEY:
        print("‚ùå OPENAI_API_KEY manquant")
        return {"articles": [], "erreur": "API key manquante"}
    
    client = OpenAI(api_key=OPENAI_API_KEY)
    date_fin = datetime.now()
    date_debut = date_fin - timedelta(days=7)
    
    # Prompt de collecte PURE optimis√©
    prompt = f"""ROBOT DE COLLECTE D'ACTUALIT√âS IA/LLM - AUCUNE ANALYSE

P√âRIODE: {date_debut.strftime('%d/%m/%Y')} au {date_fin.strftime('%d/%m/%Y')} (7 derniers jours)

SOURCES PRIORITAIRES (v√©rifier blogs/news officiels):
- Anthropic: anthropic.com/news
- OpenAI: openai.com/news | openai.com/research
- Mistral AI: mistral.ai/news
- DeepSeek: deepseekai.com
- The Hacker News (cybersecurity AI): thehackernews.com
- Hacker News AI discussions: news.ycombinator.com
- DeepLearning.AI: deeplearning.ai/the-batch
- Google AI Blog: blog.google/technology/ai
- NVIDIA AI: blogs.nvidia.com/blog/category/deep-learning
- Hugging Face: huggingface.co/blog
- Meta AI: ai.meta.com/blog
- AWS AI/ML: aws.amazon.com/blogs/machine-learning

OBJECTIF: Collecter EXACTEMENT 25 articles r√©cents (publi√©s dans les 7 derniers jours)

CONSIGNES STRICTES:
1. Collecte BRUTE uniquement - PAS de s√©lection, PAS d'analyse, PAS de synth√®se
2. V√©rifier que chaque URL est valide et accessible
3. Extraire le contenu r√©el depuis les sources (pas de sp√©culation)
4. Diversifier les sources (pas plus de 3-4 articles par source)
5. Privil√©gier les annonces officielles et articles techniques

CAT√âGORIES (choisir la plus pertinente):
- Nouveaux mod√®les LLM (sorties, benchmarks, capacit√©s)
- Agents & Agentic AI (frameworks, orchestration, autonomie)
- Multimodal (vision, audio, vid√©o int√©gr√©s aux LLM)
- Reasoning & Chain-of-Thought (o1, r√©flexion, planning)
- Open source (releases, fine-tuning, communaut√©)
- Recherche (papers, techniques, algorithmes)
- R√©gulation & Policy (lois, normes, gouvernance)
- Safety & Alignment (s√©curit√©, √©thique, red-teaming)
- Industrie & Applications (adoption entreprise, cas d'usage)
- Hardware & Infrastructure (GPUs, TPUs, optimisation)
- France & Europe (initiatives locales, startups, r√©gulation EU)
- Asie (Chine, Japon, Cor√©e - DeepSeek, etc.)

FORMAT JSON STRICT (sans markdown, sans commentaires):
{{
  "articles": [
    {{
      "titre": "Titre exact de l'article",
      "url": "https://source-officielle.com/article-complet",
      "source": "Nom de la source (ex: Anthropic, OpenAI)",
      "date_publication": "YYYY-MM-DD",
      "contenu_brut": "R√©sum√© factuel de 2-3 phrases extrait du contenu r√©el de l'article",
      "categorie_auto": "Cat√©gorie la plus pertinente parmi celles list√©es"
    }}
  ],
  "periode": {{
    "debut": "{date_debut.strftime('%Y-%m-%d')}", 
    "fin": "{date_fin.strftime('%Y-%m-%d')}"
  }},
  "nb_articles": 25
}}

EXEMPLE DE R√âSULTAT ATTENDU:
{{
  "titre": "Introducing Claude 3.5 Sonnet with improved coding abilities",
  "url": "https://www.anthropic.com/news/claude-3-5-sonnet",
  "source": "Anthropic",
  "date_publication": "2026-01-28",
  "contenu_brut": "Anthropic annonce une nouvelle version de Claude 3.5 Sonnet avec des capacit√©s de codage am√©lior√©es, atteignant 95% sur SWE-bench. Le mod√®le introduit √©galement de meilleures performances sur les t√¢ches de raisonnement math√©matique.",
  "categorie_auto": "Nouveaux mod√®les LLM"
}}

IMPORTANT:
- Retourner UNIQUEMENT le JSON (pas de texte avant/apr√®s)
- 25 articles OBLIGATOIRE (ni plus, ni moins)
- URLs compl√®tes et valides
- Dates au format YYYY-MM-DD
- Contenu factuel (pas d'opinion)"""

    print(f"üåê Lancement GPT-5.2 + web search LIVE...")
    print(f"üìÖ Recherche sur 7 jours : {date_debut.strftime('%d/%m')} - {date_fin.strftime('%d/%m')}")
    
    try:
        # Appel OpenAI Responses API (PAS de max_tokens avec GPT-5.2)
        response = client.responses.create(
            model=MODEL_RECHERCHE,
            tools=[{"type": "web_search", "external_web_access": True}],
            input=prompt
        )
        
        tokens_used = response.usage.total_tokens
        print(f"üìä Tokens utilis√©s : {tokens_used}")
        
        # Nettoyage du JSON
        json_text = response.output_text.strip()
        
        # Retirer les balises markdown si pr√©sentes
        if json_text.startswith('```'):
            lines = json_text.split('\n')
            json_text = '\n'.join(lines[1:-1]) if len(lines) > 2 else json_text
            json_text = json_text.replace('```json', '').replace('```', '').strip()
        
        # Parse JSON
        data = json.loads(json_text)
        
        # Validation basique
        if 'articles' not in data or not isinstance(data['articles'], list):
            raise ValueError("Format JSON invalide : cl√© 'articles' manquante ou invalide")
        
        # Enrichissement m√©tadonn√©es
        data['date_collecte'] = date_fin.strftime('%Y-%m-%d %H:%M:%S')
        data['model_utilise'] = MODEL_RECHERCHE
        data['tokens_utilises'] = tokens_used
        data['agent'] = "Recherche IA v3"
        data['nb_articles'] = len(data.get('articles', []))
        
        # G√©n√©ration ID unique pour chaque article
        for article in data.get('articles', []):
            hash_input = f"{article.get('url', '')}{article.get('titre', '')}"
            article['id'] = hashlib.md5(hash_input.encode()).hexdigest()[:12]
        
        print(f"‚úÖ {data['nb_articles']} articles collect√©s")
        
        # Afficher la r√©partition par cat√©gorie
        categories = {}
        for art in data['articles']:
            cat = art.get('categorie_auto', 'Non class√©')
            categories[cat] = categories.get(cat, 0) + 1
        
        print(f"üìä R√©partition par cat√©gorie :")
        for cat, count in sorted(categories.items(), key=lambda x: x[1], reverse=True):
            print(f"   ‚Ä¢ {cat}: {count}")
        
        return data
    
    except json.JSONDecodeError as e:
        print(f"‚ùå Erreur JSON : {e}")
        print(f"R√©ponse brute (500 premiers chars) :")
        print(response.output_text[:500])
        traceback.print_exc()
        raise
    
    except Exception as e:
        print(f"‚ùå Erreur : {e}")
        traceback.print_exc()
        raise

def main():
    try:
        print("=" * 80)
        print("ü§ñ AGENT RECHERCHE IA v3 - COLLECTE PURE")
        print("=" * 80)
        print(f"üìÖ P√©riode : 7 derniers jours")
        print(f"üéØ Objectif : {MAX_ARTICLES} articles EXACTEMENT")
        print(f"üåê Mod√®le : {MODEL_RECHERCHE} + web search live")
        print()
        
        data = collecter_actualites_ia()
        
        # Sauvegarde JSON
        with open(OUTPUT_JSON, 'w', encoding='utf-8') as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        print()
        print(f"‚úÖ Fichier g√©n√©r√© : {OUTPUT_JSON}")
        print(f"üìä {data['nb_articles']} articles ‚Ä¢ {data['tokens_utilises']} tokens")
        print("=" * 80)
        sys.exit(0)
    
    except Exception as e:
        print()
        print(f"‚ùå √âCHEC : {e}")
        print("=" * 80)
        sys.exit(1)

if __name__ == "__main__":
    main()
