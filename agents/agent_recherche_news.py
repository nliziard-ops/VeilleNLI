"""
Agent 2 - Recherche Web News
Mod√®le : GPT-4 Turbo (ChatGPT)
R√¥le : Collecte factuelle depuis presse nationale/internationale/locale
Sans interpr√©tation ni analyse - Restitution brute : cat√©gorie, titre, r√©sum√©, synth√®se, source+lien
"""

import os
import sys
import json
import hashlib
import traceback
from datetime import datetime, timedelta
from typing import Dict, Any
from openai import OpenAI


# ================================================================================
# CONFIGURATION
# ================================================================================

OPENAI_API_KEY = os.environ.get('OPENAI_API_KEY')

# Mod√®le ChatGPT-4 Turbo pour recherche web
MODEL_RECHERCHE = "gpt-4-turbo-preview"

# Fichier de sortie
OUTPUT_JSON = "recherche_news_brute.json"

# Sources presse (d√©finies par Nicolas)
SOURCES_NEWS = [
    # INTERNATIONAL
    "https://legrandcontinent.eu/fr/",
    "https://elpais.com/",
    "https://www.bbc.com/news",          # Ajout institutionnel
    "https://www.reuters.com",           # Ajout institutionnel
    
    # NATIONAL FRANCE
    "https://www.lefigaro.fr/",
    "https://www.lemonde.fr/",
    "https://www.monde-diplomatique.fr/",
    
    # LOCAL BRETAGNE/PAYS DE LOIRE
    "https://www.ouest-france.fr/",
    "https://www.letelegramme.fr/"
]


# ================================================================================
# RECHERCHE WEB NEWS AVEC CHATGPT-4 TURBO
# ================================================================================

def rechercher_actualites_news() -> Dict[str, Any]:
    """
    Utilise ChatGPT-4 Turbo avec web_search pour collecter
    actualit√©s depuis presse nationale/internationale/locale.
    
    Returns:
        Dictionnaire JSON avec articles cat√©goris√©s
    """
    
    if not OPENAI_API_KEY:
        print("‚ùå OPENAI_API_KEY manquante")
        return {"articles": []}
    
    print(f"üîç Cr√©ation client OpenAI pour recherche web...")
    client = OpenAI(api_key=OPENAI_API_KEY)
    
    # Calculer p√©riode (7 derniers jours)
    date_fin = datetime.now()
    date_debut = date_fin - timedelta(days=7)
    
    # Pr√©parer liste sources
    sources_text = "\n".join([f"- {source}" for source in SOURCES_NEWS])
    
    # Construire prompt de recherche factuelle
    prompt = f"""Tu es un collecteur d'informations factuelles d'actualit√©s.

**P√âRIODE** : du {date_debut.strftime('%d/%m/%Y')} au {date_fin.strftime('%d/%m/%Y')}

**SOURCES PRIORITAIRES √Ä CONSULTER** :
{sources_text}

**DISTRIBUTION G√âOGRAPHIQUE CIBLE** :
- 35% International (g√©opolitique, √©conomie mondiale, crises)
- 35% France (politique, √©conomie, soci√©t√©)
- 30% Local Bretagne/Pays de Loire/Nantes (politique locale, √©conomie r√©gionale, sports maritimes : voile, surf, kitesurf, wingfoil)

**TA MISSION - COLLECTE FACTUELLE UNIQUEMENT** :
1. Recherche les actualit√©s importantes publi√©es cette semaine sur ces sources
2. Pour chaque actualit√© trouv√©e, extrais UNIQUEMENT les faits :
   - Titre exact de l'article
   - R√©sum√© court (2-3 lignes) - FACTS ONLY
   - Contenu factuel complet (qui, quoi, quand, o√π, pourquoi, comment)
   - Source exacte (nom du m√©dia)
   - URL compl√®te
   - Date de publication

3. Cat√©gorise chaque actualit√© dans l'un de ces th√®mes :
   **INTERNATIONAL** :
   - "G√©opolitique" (conflits, diplomatie, relations internationales)
   - "√âconomie mondiale" (march√©s, commerce, crises)
   - "Environnement" (climat, biodiversit√©, catastrophes)
   
   **FRANCE** :
   - "Politique nationale" (gouvernement, lois, √©lections)
   - "√âconomie France" (entreprises, emploi, budget)
   - "Soci√©t√©" (mouvement sociaux, justice, √©ducation)
   
   **LOCAL BRETAGNE/PAYS DE LOIRE** :
   - "Politique locale" (r√©gion, d√©partement, mairies)
   - "√âconomie r√©gionale" (entreprises locales, emploi)
   - "Sports maritimes" (voile, surf, kitesurf, wingfoil, comp√©titions)
   - "Mer & littoral" (ports, p√™che, environnement marin)
   - "Culture Bretagne" (√©v√©nements, patrimoine)

**FORMAT DE SORTIE JSON - STRUCTURE OBLIGATOIRE** :
R√©ponds UNIQUEMENT avec un JSON valide suivant ce format exact :

Articles sous forme de liste avec pour chaque article :
- categorie (string)
- zone_geo (string : "International", "National" ou "Local")
- titre (string)
- resume_court (string de 2-3 lignes)
- synthese_complete (string factuelle)
- source (string, nom du m√©dia)
- url (string, URL compl√®te)
- date_publication (string format YYYY-MM-DD)

Ajoute aussi :
- periode avec debut et fin
- repartition avec international, national, local (nombres)
- sources_consultees (liste)

**CONSIGNES CRITIQUES** :
- Vise 15-20 actualit√©s maximum (limite tokens)
- Respecte la distribution : ~35% international, ~35% national, ~30% local
- UNIQUEMENT des faits v√©rifiables
- AUCUNE interpr√©tation, analyse, opinion
- Citations exactes quand pertinent
- URLs compl√®tes obligatoires
- Pour le local : focus Nantes, Brest, Belle-√éle-en-Mer, Le Palais
- Sports maritimes = priorit√© (voile, surf, kitesurf, wingfoil)

**IMPORTANT** :
Tu es un COLLECTEUR, pas un ANALYSTE.
Tu retranscris les informations telles qu'elles apparaissent.

Utilise la fonction web_search pour acc√©der aux sites de presse.
G√©n√®re le JSON maintenant, sans pr√©ambule."""

    print("üåê Lancement recherche web ChatGPT-4 Turbo...")
    
    try:
        # Appel API ChatGPT-4 Turbo
        response = client.chat.completions.create(
            model=MODEL_RECHERCHE,
            messages=[
                {
                    "role": "system",
                    "content": "Tu es un collecteur d'informations factuelles. Tu r√©ponds UNIQUEMENT en JSON valide, sans markdown, sans commentaires. Tu utilises web_search pour acc√©der aux sites."
                },
                {
                    "role": "user",
                    "content": prompt
                }
            ],
            temperature=0.1,  # Tr√®s factuel
            max_tokens=4000   # R√âDUIT : max 4096 pour gpt-4-turbo-preview
        )
        
        print(f"üìä Tokens utilis√©s : {response.usage.total_tokens} (prompt: {response.usage.prompt_tokens}, completion: {response.usage.completion_tokens})")
        
        # Co√ªt GPT-4 Turbo
        cost_input = (response.usage.prompt_tokens / 1000) * 0.01
        cost_output = (response.usage.completion_tokens / 1000) * 0.03
        cost_total = cost_input + cost_output
        print(f"üí∞ Co√ªt estim√© : ${cost_total:.4f}")
        
        # Extraire JSON
        json_text = response.choices[0].message.content.strip()
        
        # Nettoyer markdown
        if json_text.startswith('```'):
            lines = json_text.split('\n')
            json_text = '\n'.join(lines[1:-1]) if len(lines) > 2 else json_text
            json_text = json_text.replace('```json', '').replace('```', '').strip()
        
        print(f"üìù Parsing JSON ({len(json_text)} caract√®res)...")
        
        data = json.loads(json_text)
        
        # Ajouter m√©tadonn√©es
        data['date_collecte'] = date_fin.strftime('%Y-%m-%d')
        data['model_utilise'] = MODEL_RECHERCHE
        data['agent'] = "Recherche News"
        
        # V√©rifier structure
        if 'periode' not in data:
            data['periode'] = {
                'debut': date_debut.strftime('%Y-%m-%d'),
                'fin': date_fin.strftime('%Y-%m-%d')
            }
        
        # Calculer r√©partition g√©ographique
        if 'repartition' not in data:
            data['repartition'] = {'international': 0, 'national': 0, 'local': 0}
        
        for article in data.get('articles', []):
            # G√©n√©rer ID unique
            hash_input = f"{article.get('url', '')}{article.get('titre', '')}"
            article['id'] = hashlib.md5(hash_input.encode()).hexdigest()[:12]
            
            # Compter r√©partition
            zone = article.get('zone_geo', 'National')
            if zone == 'International':
                data['repartition']['international'] += 1
            elif zone == 'Local':
                data['repartition']['local'] += 1
            else:
                data['repartition']['national'] += 1
        
        total_articles = len(data.get('articles', []))
        print(f"‚úÖ Recherche termin√©e : {total_articles} articles")
        if total_articles > 0:
            print(f"üåç R√©partition : International {data['repartition']['international']}, National {data['repartition']['national']}, Local {data['repartition']['local']}")
        
        return data
    
    except json.JSONDecodeError as e:
        print(f"‚ùå Erreur parsing JSON : {e}")
        print(f"R√©ponse brute (premiers 500 car) : {json_text[:500]}...")
        raise
    
    except Exception as e:
        print(f"‚ùå Erreur ChatGPT-4 Turbo : {e}")
        traceback.print_exc()
        raise


# ================================================================================
# SAUVEGARDE JSON
# ================================================================================

def sauvegarder_json(data: Dict[str, Any], filepath: str) -> None:
    """
    Sauvegarde le JSON de recherche brute
    """
    print(f"üíæ Sauvegarde du JSON dans {filepath}...")
    
    with open(filepath, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    
    file_size = os.path.getsize(filepath)
    print(f"‚úÖ JSON sauvegard√© : {filepath}")
    print(f"üìä Taille : {file_size} octets ({file_size / 1024:.2f} KB)")


# ================================================================================
# MAIN
# ================================================================================

def main():
    """Point d'entr√©e principal de l'agent recherche News"""
    
    try:
        print("=" * 80)
        print("ü§ñ AGENT 2 - RECHERCHE WEB NEWS (ChatGPT-4 Turbo)")
        print("=" * 80)
        print(f"‚è∞ Ex√©cution : {datetime.now().strftime('%d/%m/%Y %H:%M:%S')}")
        print(f"üìÇ R√©pertoire de travail : {os.getcwd()}")
        print()
        
        # V√©rifier cl√© API
        print("üîë V√©rification cl√© API...")
        if not OPENAI_API_KEY:
            print("‚ùå ERREUR CRITIQUE : OPENAI_API_KEY manquante")
            sys.exit(1)
        else:
            print(f"‚úÖ OPENAI_API_KEY pr√©sente ({OPENAI_API_KEY[:10]}...)")
        
        print()
        
        # Recherche web
        print("üì° RECHERCHE WEB FACTUELLE NEWS")
        print("-" * 80)
        print("Sources presse :")
        for source in SOURCES_NEWS:
            print(f"  ‚Ä¢ {source}")
        print()
        print("Distribution cible : 35% International, 35% National, 30% Local")
        print()
        
        data = rechercher_actualites_news()
        print()
        
        # Sauvegarde
        print("üíæ SAUVEGARDE JSON")
        print("-" * 80)
        sauvegarder_json(data, OUTPUT_JSON)
        print()
        
        # R√©sum√©
        print("=" * 80)
        print("‚úÖ AGENT 2 RECHERCHE NEWS TERMIN√â")
        print("=" * 80)
        print(f"üìä {len(data.get('articles', []))} articles collect√©s")
        print(f"üìÇ Fichier JSON : {OUTPUT_JSON}")
        print(f"üîó Pr√™t pour Agent 4 (Synth√®se News)")
        print()
        
        sys.exit(0)
    
    except KeyboardInterrupt:
        print("\n‚ö†Ô∏è  Interruption manuelle (Ctrl+C)")
        sys.exit(130)
    
    except Exception as e:
        print("\n" + "=" * 80)
        print("‚ùå ERREUR FATALE")
        print("=" * 80)
        print(f"Type : {type(e).__name__}")
        print(f"Message : {e}")
        print("\nTraceback :")
        traceback.print_exc()
        print("=" * 80)
        sys.exit(1)


if __name__ == "__main__":
    main()
